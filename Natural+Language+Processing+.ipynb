{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#source: https://pythonprogramming.net/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stentence Tokenization \n",
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n",
      "Word Tokenization \n",
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#tokenzier word and sentence\n",
    "#lexicon words and meanings \n",
    "#corporas body of text ex: presidental speechs \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
    "\n",
    "print (\"Stentence Tokenization \")\n",
    "print(sent_tokenize(EXAMPLE_TEXT))\n",
    "print (\"Word Tokenization \")\n",
    "print(word_tokenize(EXAMPLE_TEXT))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maury', ',', 'however', ',', 'recognized', 'great', 'depths', 'Washington', 'Austrians', 'Pola', 'surest', 'guarantee', 'bottom', 'reached', 'bring', '1890-1893', ',', 'latter', 'carrying', 'investigations', 'Red', 'sample', 'deposit']\n",
      "['Maury', ',', 'however', ',', 'recognized', 'great', 'depths', 'Washington', 'Austrians', 'Pola', 'surest', 'guarantee', 'bottom', 'reached', 'bring', '1890-1893', ',', 'latter', 'carrying', 'investigations', 'Red', 'sample', 'deposit']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example_sent = \"Maury, however, recognized that in great depths on the  Washington  and by the Austrians on the  Pola  the surest guarantee of bottom having been reached was to bring in 1890-1893, the latter carrying the investigations to the Red up a sample of the deposit\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "#print(word_tokens)\n",
    "print(filtered_sentence)\n",
    "\n",
    "\n",
    "#oneliner\n",
    "filtered_sentence1 = [ w for w in word_tokens if not w in stop_words ]\n",
    "print(filtered_sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spookier\n",
      "spooki\n",
      "spooki\n",
      "spook\n",
      "spoonbil\n",
      "\n",
      "\n",
      "there\n",
      "is\n",
      "noth\n",
      "you\n",
      "can\n",
      "not\n",
      "not\n",
      "do\n",
      "with\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "#stemming : take words and root stem Many variations of words carry the same meaning The reason why we stem is to shorten the lookup, and normalize sentences.\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "example_words = ['spookier', 'spookiness', 'spooky', 'spook', 'spoonbill']\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    "    \n",
    "new_text = \"There is nothing you cannot not do with python\"\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('BILL', 'NNP'), ('CLINTON', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('27', 'CD'), (',', ','), ('2000', 'CD'), ('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Mr.', 'NNP'), ('Vice', 'NNP'), ('President', 'NNP'), (',', ','), ('Members', 'NNP'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('honored', 'VBD'), ('guests', 'NNS'), (',', ','), ('my', 'PRP$'), ('fellow', 'JJ'), ('Americans', 'NNPS'), (':', ':'), ('We', 'PRP'), ('are', 'VBP'), ('fortunate', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('alive', 'JJ'), ('at', 'IN'), ('this', 'DT'), ('moment', 'NN'), ('in', 'IN'), ('history', 'NN'), ('.', '.')]\n",
      "[('Never', 'RB'), ('before', 'RB'), ('has', 'VBZ'), ('our', 'PRP$'), ('Nation', 'NNP'), ('enjoyed', 'NN'), (',', ','), ('at', 'IN'), ('once', 'RB'), (',', ','), ('so', 'RB'), ('much', 'JJ'), ('prosperity', 'NN'), ('and', 'CC'), ('social', 'JJ'), ('progress', 'NN'), ('with', 'IN'), ('so', 'RB'), ('little', 'JJ'), ('internal', 'JJ'), ('crisis', 'NN'), ('and', 'CC'), ('so', 'RB'), ('few', 'JJ'), ('external', 'JJ'), ('threats', 'NNS'), ('.', '.')]\n",
      "[('Never', 'RB'), ('before', 'RB'), ('have', 'VBP'), ('we', 'PRP'), ('had', 'VBD'), ('such', 'JJ'), ('a', 'DT'), ('blessed', 'JJ'), ('opportunity', 'NN'), ('and', 'CC'), (',', ','), ('therefore', 'RB'), (',', ','), ('such', 'PDT'), ('a', 'DT'), ('profound', 'JJ'), ('obligation', 'NN'), ('to', 'TO'), ('build', 'VB'), ('the', 'DT'), ('more', 'RBR'), ('perfect', 'JJ'), ('Union', 'NNP'), ('of', 'IN'), ('our', 'PRP$'), ('Founders', 'NNS'), (\"'\", 'POS'), ('dreams', 'NNS'), ('.', '.')]\n",
      "[('We', 'PRP'), ('begin', 'VBP'), ('the', 'DT'), ('new', 'JJ'), ('century', 'NN'), ('with', 'IN'), ('over', 'IN'), ('20', 'CD'), ('million', 'CD'), ('new', 'JJ'), ('jobs', 'NNS'), (';', ':'), ('the', 'DT'), ('fastest', 'JJS'), ('economic', 'JJ'), ('growth', 'NN'), ('in', 'IN'), ('more', 'JJR'), ('than', 'IN'), ('30', 'CD'), ('years', 'NNS'), (';', ':'), ('the', 'DT'), ('lowest', 'JJS'), ('unemployment', 'NN'), ('rates', 'NNS'), ('in', 'IN'), ('30', 'CD'), ('years', 'NNS'), (';', ':'), ('the', 'DT'), ('lowest', 'JJS'), ('poverty', 'NN'), ('rates', 'NNS'), ('in', 'IN'), ('20', 'CD'), ('years', 'NNS'), (';', ':'), ('the', 'DT'), ('lowest', 'JJS'), ('African-', 'JJ'), ('American', 'JJ'), ('and', 'CC'), ('Hispanic', 'JJ'), ('unemployment', 'NN'), ('rates', 'NNS'), ('on', 'IN'), ('record', 'NN'), (';', ':'), ('the', 'DT'), ('first', 'JJ'), ('back-to-', 'JJ'), ('back', 'NN'), ('surpluses', 'NNS'), ('in', 'IN'), ('42', 'CD'), ('years', 'NNS'), (';', ':'), ('and', 'CC'), ('next', 'JJ'), ('month', 'NN'), (',', ','), ('America', 'NNP'), ('will', 'MD'), ('achieve', 'VB'), ('the', 'DT'), ('longest', 'JJS'), ('period', 'NN'), ('of', 'IN'), ('economic', 'JJ'), ('growth', 'NN'), ('in', 'IN'), ('our', 'PRP$'), ('entire', 'JJ'), ('history', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('have', 'VBP'), ('built', 'VBN'), ('a', 'DT'), ('new', 'JJ'), ('economy', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#part of Speech Tagging \n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text = state_union.raw(\"1993-Clinton.txt\")\n",
    "sample_text = state_union.raw(\"2000-Clinton.txt\")\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunking\n",
    "#main goals of chunking is to group into what are known as \"noun phrases.\" These are phrases of one or more words that contain a noun, maybe some descriptive words, maybe a verb, and maybe something like an adverb. The idea is to group nouns with the words that are in relation to them.\n",
    "#+ = match 1 or more\n",
    "#? = match 0 or 1 repetitions.\n",
    "#* = match 0 or MORE repetitions\t  \n",
    "#. = Any character except a new line\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"1993-Clinton.txt\")\n",
    "sample_text = state_union.raw(\"2000-Clinton.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
